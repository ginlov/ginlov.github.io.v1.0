<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Variational Autoencoder explanation | Giang Vu Long</title> <meta name="author" content="Giang Long Vu"/> <meta name="description" content="Math and ideas behind VAE."/> <meta name="keywords" content="deep learning, generative model, diffusion, machine learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/prof_pic.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ginlov.github.io/blog/2022/vae/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Variational Autoencoder explanation",
      "description": "Math and ideas behind VAE.",
      "published": "August 25, 2022",
      "page_id": "ginlov.github.io/blog/2022/vae/",
      "authors": [
        {
          "author": "Giang Vu Long",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Hanoi",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://ginlov.github.io/">Giang Vu Long</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/ginlovtheorem/">GinlovTheorem</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Variational Autoencoder explanation</h1> <p><img src="https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fginlov%2Fginlov.github.io/blog/2022/vae/&amp;label=Visitors&amp;countColor=%23263759"> </p> <p>Math and ideas behind VAE.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#likelihood-based-generative-models">Likelihood-based generative models</a></div> <div><a href="#latent-variable-models">Latent variable models</a></div> <div><a href="#variational-inference-and-evidence-lower-bound">Variational inference and evidence lower bound</a></div> <div><a href="#reparameterization-trick">Reparameterization trick</a></div> <div><a href="#connection-to-auto-encoder">Connection to auto-encoder</a></div> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <h2 id="likelihood-based-generative-models">Likelihood-based generative models</h2> <p>Generative Model is one of two model types in deep learning, which has the ability of generating data by sampling from the approximated distribution of data. That process requires the model to understand or have the ability to simulate the distribution of the given data. To do that, one straight forward approach is modeling the density function of data by a neural network \(p_{\theta}(x)\). That class of models called likelihood-based generative models. The objective is to maximize the likelihood function indexed by a set of paramters \(\theta\):</p> \[\begin{align*} \max_{\theta} \sum_{i} \log p_{\theta}(x^{(i)}) \end{align*}\] <p>The problem, from here, is to choose a proper architecture that can not only <strong>efficiently calculate the likelihood</strong> \(p_{\theta}(x)\) for training but also <strong>easily sample</strong> from. There are multiple ways to achieve that, such as autoregressive models <d-cite key="khajenezhad2020masked"></d-cite> <d-cite key="van2016pixel"></d-cite> <d-cite key="van2016conditional"></d-cite> <d-cite key="salimans2017pixelcnn++"></d-cite>, using an assumption of time-series data, or flow model <d-cite key="dinh2016density"></d-cite> <d-cite key="kingma2018glow"></d-cite> <d-cite key="ho2019flow++"></d-cite>. In this blog, we explore another approach which does not calculate exactly the likelihood but approximates it by an inferencing technique known as variational inference.</p> <hr> <h2 id="latent-variable-models">Latent variable models</h2> <p>VAE <d-cite key="kingma2013auto"></d-cite> is inspired by latent variable models, which rely on an assumption about data - there is a compact representation of a data point in a lower-dimensional space. The representation is known as latent code. That means, again, one can encode a data set in the original space into a set of code in the latent space (look at the below image) while maintaining the properties of data distribution.</p> \[p_{\theta}(x) = \mathbb{E}_{z \sim p_{Z}} p_{\theta}(x\|z)\] <p>Technically, in case of descrete latent variable \(z\), \(p_{\theta}(x)\) can be transformed to \(\sum_{z}p_{Z}(z)p_{\theta}(x\|z)\). Unforturnately, in many real-world problems, \(z\) is continuous. Then, the challenge is calculating \(\int_{z}p_{Z}(z)p_{\theta}(x\|z)dz\). In that case, likelihood term \(p_{\theta}(x)\) can not be exactly calculated but approximated by some techniques. In next section, we will explore one of that techniques called variational inference.</p> <hr> <h2 id="variational-inference-and-evidence-lower-bound">Variational inference and evidence lower bound</h2> <p>Applying Jensens inequality to the (above) log likelihood, we have the evidence lower bound: \(\begin{align*} \log{p_{\theta}(x^{(i)})} &amp;= \log \int_{z} p_{\theta}(x^{(i)}, z) dz \\ &amp;= \log \int_{z} p_{\theta}(x^{(i)}, z) \frac{q_{\phi}(z|x^{(i)})}{q_{\phi}(z|x^{(i)})} dz \\ &amp;= \log \mathbb{E}_{q_{\phi}(z|x^{(i)})} \frac{p_{\theta}(x^{(i)}, z)}{q_{\phi}(z|x^{(i)})} \\ &amp;\geq \mathbb{E}_{q_{\phi}(z|x^{(i)})} \log \left[ \frac{p_{\theta}(x^{(i)}, z)}{q_{\phi}(z)} \right] \textnormal{(Jensen inequality)}\\ &amp;= \mathbb{E}_{q_{\phi}(z|x^{(i)})} \left[\log p_{\theta}(x^{(i)}, z) - \log q_{\phi}(z | x^{(i)}) \right]\\ &amp;= \mathcal{L}(\theta, \phi, x^{(i)}) \end{align*}\)</p> <p>Where \(q_{\phi}(z)\) is called recognition model and \(\mathcal{L}(\theta, \phi, x^{(i)})\) is called evidence lower bound of the log likelihood function. To calculate ELBO, it is essential to reckon the expectation over recognition distribution. Prior approaches, such as Monte Carlo sampling \(\mathbb{E}_{q_{\phi}}[f(z)] \approx \frac{1}{K} \sum_{i=1}^{K} f(z^{(i)})\) is high variance which leads to unreasonable result. In the next section, a reparameterization trick is introduced to solve this problem.</p> <hr> <h2 id="reparameterization-trick">Reparameterization trick</h2> <p>With \(z\) is a random variable, and \(z \sim q_{\phi}(z|x)\), it is possible to express \(z\) as a deterministic variable \(z=g_{\phi}(\epsilon, x)\), where \(\epsilon\) is an auxiliary variable with independent marginal \(p(\epsilon)\).</p> <p>For example, assume that \(z \sim q_{\phi}(z, x) = \mathcal{N}(\mu, \sigma^{2})\). \(z\) can be expressed as \(z=\mu + \sigma \epsilon\) where \(\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\).</p> <p>Using above trick, the ELBO can be re-written as:</p> \[\begin{align*} \mathcal{L}(\theta, \phi, x^{(i)}) &amp;= \mathbb{E}_{q_{\phi}(z|x^{(i)})} \left[\log p_{\theta}(x^{(i)}, z) - \log q_{\phi}(z | x^{(i)}) \right]\\ &amp;= \frac{1}{L} \sum_{l=1}^{L} \left[ \log p_{\theta}(x^{(i)}, g_{\phi}(x^{(i)}, \epsilon^{(l)})) - \log q_{\phi}(g_{\phi}(x^{(i)}, \epsilon^{(l)}) | x^{(i)})\right] \end{align*}\] <p>Where \(\epsilon^{(l)} \sim p(\epsilon)\). Since \(p(\epsilon)\) is independent, sampling is now easy with low variance. From here, optimizing ELBO is as normal as optimizing other loss function using gradient descent. In the next section, another perspective of this loss function is investigated and illustrated the connection to auto-encoder.</p> <hr> <h2 id="connection-to-auto-encoder">Connection to auto-encoder</h2> <p>Further transforming the evidence lower bound give us another view of this loss function:</p> \[\begin{align*} \mathcal{L}(\theta, \phi, x^{(i)}) &amp;= \mathbb{E}_{q_{\phi}(z|x^{(i)})} \left[\log p_{\theta}(x^{(i)}, z) - \log q_{\phi}(z | x^{(i)}) \right]\\ &amp;= \mathbb{E}_{q_{\phi}(z|x^{(i)})} \left[\log p_{\theta}(x^{(i)} | z) + \log p_{\theta}(z) - \log q_{\phi}(z | x^{(i)}) \right]\\ &amp;= \mathbb{E}_{q_{\phi}(z|x^{(i)})} \log p_{\theta}(x^{(i)}|z) - \mathbb{E}_{q_{\phi}(z|x^{(i)})} \log \frac{q_{\phi}(z | x^{(i)})}{p_{\theta}(z)} \\ &amp;= \mathbb{E}_{q_{\phi}(z|x^{(i)})} \log p_{\theta}(x^{(i)}|z) - \mathrm{D}_{KL}(q_{\phi}(z|x^{(i)}) || p_{\theta}(z)) \end{align*}\] <p>The first term is the reconstruction loss and the second term can be thought as a regularization term. Because of that, this model can be thought as the variational auto-encoder.</p> <p>Optimizing loss function using this view is similar to the mentioned above method.</p> <hr> <h2 id="conclusion">Conclusion</h2> <p>Auto-encoding Variational Bayes introduced the recognition model \(q_{\phi}(z|x)\) to approximate the true posterior \(p_{\theta}(z|x)\) which is intractable in general case. In addition, to efficiently calculate the loss term, AVB also proposed a reparameterization trick which allow sampling from recognition model \(q_{\phi}(z|x)\) easily with low variance.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Giang Long Vu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: August 30, 2023. </div> </footer> </body> <d-bibliography src="/assets/bibliography/vaetheory.bib"> </d-bibliography> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> </html>